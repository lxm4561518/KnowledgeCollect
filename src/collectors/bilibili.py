import os
import re
import tempfile
import http.cookiejar
from yt_dlp import YoutubeDL
from ..auth.cookies import cookie_header_for
from .common import load_cookies_safely
from .browser_fallback import get_cookies as get_playwright_cookies


def _save_cookies_to_netscape_file(jar, filename):
    mozilla_jar = http.cookiejar.MozillaCookieJar(filename)
    for cookie in jar:
        mozilla_jar.set_cookie(cookie)
    mozilla_jar.save()


def _save_playwright_cookies_to_netscape(cookies: list, filename: str):
    with open(filename, "w", encoding="utf-8") as f:
        f.write("# Netscape HTTP Cookie File\n")
        f.write("# This file is generated by Playwright\n\n")
        for c in cookies:
            domain = c.get("domain", "")
            flag = "TRUE" if domain.startswith(".") else "FALSE"
            path = c.get("path", "/")
            secure = "TRUE" if c.get("secure", False) else "FALSE"
            expiration = int(c.get("expires", 0))
            if expiration < 0:
                expiration = 0
            name = c.get("name", "")
            value = c.get("value", "")
            f.write(f"{domain}\t{flag}\t{path}\t{secure}\t{expiration}\t{name}\t{value}\n")


def _read_subtitles_text(temp_dir: str) -> str:
    lines = []
    for f in os.listdir(temp_dir):
        if f.lower().endswith((".srt", ".ass", ".vtt")):
            p = os.path.join(temp_dir, f)
            with open(p, encoding="utf-8", errors="ignore") as fh:
                for line in fh:
                    if re.match(r"^\d+$", line.strip()):
                        continue
                    if re.match(r"^\d{2}:\d{2}:\d{2}", line.strip()):
                        continue
                    if line.strip() and not line.strip().startswith(('{', 'Dialogue:')):
                        lines.append(line.strip())
    return "\n".join(lines)


def collect(url: str) -> dict:
    temp_dir = tempfile.mkdtemp(prefix="bili_")
    out = os.path.join(temp_dir, "%(title)s-%(id)s.%(ext)s")
    
    # Try to load cookies safely and save to Netscape format for yt-dlp
    cookie_file = None
    try:
        # Assuming domain from url or generic .bilibili.com
        domain = ".bilibili.com"
        print(f"Loading cookies for {domain}...")
        try:
            jar = load_cookies_safely(domain)
            cookie_file = os.path.join(temp_dir, "cookies.txt")
            _save_cookies_to_netscape_file(jar, cookie_file)
            print(f"Saved cookies to {cookie_file}")
        except Exception as e:
            print(f"Standard cookie load failed, trying Playwright: {e}")
            cookies = get_playwright_cookies("https://www.bilibili.com")
            if cookies:
                cookie_file = os.path.join(temp_dir, "cookies.txt")
                _save_playwright_cookies_to_netscape(cookies, cookie_file)
                print(f"Saved Playwright cookies to {cookie_file}")
            else:
                print("Playwright failed to get cookies")
    except Exception as e:
        print(f"Could not load/save cookies: {e}")

    ydl_opts = {
        "format": "bestaudio/best",
        "outtmpl": out,
        "writesubtitles": True,
        "writeautomaticsub": True,
        "subtitleslangs": ["zh", "zh-CN", "zh-Hans", "zh-Hant"],
        "skip_download": True,
        "noplaylist": True,
        "http_headers": {
            "Referer": "https://www.bilibili.com",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
        },
        "retries": 10,
        "fragment_retries": 10,
        "verbose": True,
    }
    
    if cookie_file:
        ydl_opts["cookiefile"] = cookie_file
    else:
        # Fallback to old method
        ydl_opts["cookiesfrombrowser"] = ("edge", None, None)
        cookie = cookie_header_for(url)
        if cookie:
            ydl_opts["http_headers"]["Cookie"] = cookie

    info = None
    try:
        with YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
    except Exception as e:
        print(f"yt-dlp failed with primary options: {e}")
        # fallback try chrome profile if not already using cookiefile
        if not cookie_file:
            ydl_opts["cookiesfrombrowser"] = ("chrome", None, None)
            try:
                with YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(url, download=True)
            except Exception:
                info = None
    
    title = info.get("title", "") if info else ""
    text = _read_subtitles_text(temp_dir)
    
    # Check if we have an audio file downloaded by yt-dlp
    audio_file = None
    for f in os.listdir(temp_dir):
        if f.endswith((".m4a", ".mp3", ".wav", ".webm")):
            audio_file = os.path.join(temp_dir, f)
            break

    # Fallback to transcription if no subtitles found
    if not text:
        if audio_file:
            print(f"No subtitles found, but audio file exists: {audio_file}. Transcribing...")
            try:
                from ..transcribe.transcriber import Transcriber
                tr = Transcriber(model_size="base")
                text = tr.transcribe(audio_file)
                print("Transcription complete.")
            except Exception as e:
                print(f"Transcription failed: {e}")
        elif info:
            print("No subtitles and no audio file from yt-dlp. Attempting fallback to bilibili-api-python...")
            # Trigger fallback below by setting info to None or handling it explicitly
            # We will let the code fall through to the API fallback if text is still empty
            pass

    # Retry logic for 412 error or missing content using bilibili-api-python
    if not text:
        print("Content missing. Attempting to use bilibili-api-python to get audio stream...")
        try:
            import asyncio
            from bilibili_api import video, Credential
            
            # We need to extract bvid from url
            # https://www.bilibili.com/video/BV1rg411L7sg/?...
            bvid = None
            match = re.search(r"BV[a-zA-Z0-9]+", url)
            if match:
                bvid = match.group(0)
            
            if bvid:
                # Use asyncio.run() which handles loop setup/teardown and defaults to ProactorEventLoop on Windows
                async def get_data_via_api():
                    # Try to load credentials from browser cookies
                    from bilibili_api import Credential
                    cred = None
                    try:
                        # Try to load from data/cookies/bilibili.json first
                        cookie_json_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "data", "cookies", "bilibili.json"))
                        if os.path.exists(cookie_json_path):
                            print(f"Loading cookies from {cookie_json_path}")
                            import json
                            with open(cookie_json_path, "r", encoding="utf-8") as f:
                                cookies_list = json.load(f)
                            
                            sessdata = None
                            bili_jct = None
                            buvid3 = None
                            dedeuserid = None
                            
                            for c in cookies_list:
                                if c.get("name") == "SESSDATA":
                                    sessdata = c.get("value")
                                elif c.get("name") == "bili_jct":
                                    bili_jct = c.get("value")
                                elif c.get("name") == "buvid3":
                                    buvid3 = c.get("value")
                                elif c.get("name") == "DedeUserID":
                                    dedeuserid = c.get("value")
                                    
                            if sessdata and bili_jct:
                                print(f"Loaded Bilibili credentials from JSON (SESSDATA found, UserID: {dedeuserid})")
                                cred = Credential(sessdata=sessdata, bili_jct=bili_jct, buvid3=buvid3, dedeuserid=dedeuserid)
                        
                        if not cred:
                            # Try to load from browser_cookie3
                            cj = load_cookies_safely(".bilibili.com")
                            sessdata = None
                            bili_jct = None
                            buvid3 = None
                            dedeuserid = None
                            for c in cj:
                                if c.name == "SESSDATA":
                                    sessdata = c.value
                                elif c.name == "bili_jct":
                                    bili_jct = c.value
                                elif c.name == "buvid3":
                                    buvid3 = c.value
                                elif c.name == "DedeUserID":
                                    dedeuserid = c.value
                            
                            if sessdata and bili_jct:
                                print(f"Loaded Bilibili credentials (SESSDATA found, UserID: {dedeuserid})")
                                cred = Credential(sessdata=sessdata, bili_jct=bili_jct, buvid3=buvid3, dedeuserid=dedeuserid)
                            else:
                                print("Bilibili cookies found but missing SESSDATA/bili_jct.")
                    except Exception as e:
                        print(f"Failed to load Bilibili cookies for API: {e}")

                    v = video.Video(bvid=bvid, credential=cred)
                    # We need to get download url
                    # detect cid first
                    try:
                        info_api = await v.get_info()
                        # Debug subtitles
                        # import json
                        # print(f"Subtitle info: {json.dumps(info_api.get('subtitle'), ensure_ascii=False)}")
                    except Exception as e:
                        print(f"bilibili-api get_info failed: {e}")
                        return "", "", ""
                        
                    title_api = info_api.get("title", "")
                    
                    # Try to get subtitles directly from API
                    subtitle_text = ""
                    if "subtitle" in info_api and "list" in info_api["subtitle"]:
                        subs = info_api["subtitle"]["list"]
                        if subs:
                            # Prefer zh-CN
                            target_sub = next((s for s in subs if s.get("lan") in ["zh-CN", "zh-Hans", "zh"]), None)
                            if not target_sub:
                                target_sub = subs[0]
                            
                            sub_url = target_sub.get("subtitle_url")
                            if sub_url:
                                print(f"Found subtitle URL: {sub_url}")
                                # Fetch sub_url (json format usually)
                                import aiohttp
                                async with aiohttp.ClientSession() as session:
                                    async with session.get(sub_url) as resp:
                                        if resp.status == 200:
                                            sub_data = await resp.json()
                                            # Parse BCC/JSON subtitles
                                            # {'body': [{'from': 0.1, 'to': 2.5, 'content': '...'}, ...]}
                                            if "body" in sub_data:
                                                subtitle_text = "\n".join([item["content"] for item in sub_data["body"]])

                    # Get audio url
                    audio_url = None
                    try:
                        url_data = await v.get_download_url(page_index=0)
                        if "dash" in url_data:
                            audios = url_data["dash"].get("audio", [])
                            if audios:
                                audio_url = audios[0].get("baseUrl") or audios[0].get("backupUrl", [None])[0]
                        elif "durl" in url_data:
                            audio_url = url_data["durl"][0]["url"]
                    except Exception as e:
                        print(f"bilibili-api get_download_url failed: {e}")
                         
                    return title_api, audio_url, subtitle_text

                title_api, audio_url, subtitle_text_api = asyncio.run(get_data_via_api())
                
                if title_api:
                    title = title_api
                
                if subtitle_text_api:
                    print("Successfully retrieved subtitles via API.")
                    text = subtitle_text_api
                elif audio_url:
                    print(f"Got audio URL via API: {audio_url[:50]}...")
                    # Download audio manually
                    audio_path = os.path.join(temp_dir, "audio_temp.m4a")
                    # Use requests or cloudscraper to download
                    import cloudscraper
                    scraper = cloudscraper.create_scraper()
                    headers = {
                        "Referer": "https://www.bilibili.com",
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                    # Streaming download
                    with scraper.get(audio_url, headers=headers, stream=True) as r:
                        r.raise_for_status()
                        with open(audio_path, "wb") as f:
                            for chunk in r.iter_content(chunk_size=8192):
                                f.write(chunk)
                    
                    print(f"Downloaded audio to {audio_path}. Transcribing...")
                    from ..transcribe.transcriber import Transcriber
                    tr = Transcriber(model_size="base")
                    text = tr.transcribe(audio_path)
                    print(f"Transcription complete.")

        except Exception as e:
            print(f"bilibili-api-python fallback failed: {e}")

    # Fallback to transcription if no subtitles found (and yt-dlp worked partially or we just downloaded audio)

    if not text and info:
        desc = info.get("description", "") or info.get("webpage_url_basename", "")
        text = desc
    if not text:
        try:
            from .common import fetch_html
            from bs4 import BeautifulSoup
            html = fetch_html(url)
            soup = BeautifulSoup(html, "html.parser")
            if not title:
                title = soup.title.string if soup.title else ""
            meta_desc = soup.find("meta", attrs={"name": "description"})
            if meta_desc and meta_desc.get("content"):
                text = meta_desc["content"]
        except Exception:
            pass
    return {"title": title, "text": text}


    
