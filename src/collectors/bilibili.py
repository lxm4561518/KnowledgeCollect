import os
import re
import tempfile
import http.cookiejar
from yt_dlp import YoutubeDL
from ..auth.cookies import cookie_header_for
from .common import load_cookies_safely
from .browser_fallback import get_cookies as get_playwright_cookies


def _save_cookies_to_netscape_file(jar, filename):
    mozilla_jar = http.cookiejar.MozillaCookieJar(filename)
    for cookie in jar:
        mozilla_jar.set_cookie(cookie)
    mozilla_jar.save()


def _save_playwright_cookies_to_netscape(cookies: list, filename: str):
    with open(filename, "w", encoding="utf-8") as f:
        f.write("# Netscape HTTP Cookie File\n")
        f.write("# This file is generated by Playwright\n\n")
        for c in cookies:
            domain = c.get("domain", "")
            flag = "TRUE" if domain.startswith(".") else "FALSE"
            path = c.get("path", "/")
            secure = "TRUE" if c.get("secure", False) else "FALSE"
            expiration = int(c.get("expires", 0))
            if expiration < 0:
                expiration = 0
            name = c.get("name", "")
            value = c.get("value", "")
            f.write(f"{domain}\t{flag}\t{path}\t{secure}\t{expiration}\t{name}\t{value}\n")


def _read_subtitles_text(temp_dir: str) -> str:
    lines = []
    for f in os.listdir(temp_dir):
        if f.lower().endswith((".srt", ".ass", ".vtt")):
            p = os.path.join(temp_dir, f)
            with open(p, encoding="utf-8", errors="ignore") as fh:
                for line in fh:
                    if re.match(r"^\d+$", line.strip()):
                        continue
                    if re.match(r"^\d{2}:\d{2}:\d{2}", line.strip()):
                        continue
                    if line.strip() and not line.strip().startswith(('{', 'Dialogue:')):
                        lines.append(line.strip())
    return "\n".join(lines)


def collect(url: str) -> dict:
    temp_dir = tempfile.mkdtemp(prefix="bili_")
    out = os.path.join(temp_dir, "%(title)s-%(id)s.%(ext)s")
    
    # Try to load cookies safely and save to Netscape format for yt-dlp
    cookie_file = None
    try:
        # Assuming domain from url or generic .bilibili.com
        domain = ".bilibili.com"
        print(f"Loading cookies for {domain}...")
        try:
            jar = load_cookies_safely(domain)
            cookie_file = os.path.join(temp_dir, "cookies.txt")
            _save_cookies_to_netscape_file(jar, cookie_file)
            print(f"Saved cookies to {cookie_file}")
        except Exception as e:
            print(f"Standard cookie load failed, trying Playwright: {e}")
            cookies = get_playwright_cookies("https://www.bilibili.com")
            if cookies:
                cookie_file = os.path.join(temp_dir, "cookies.txt")
                _save_playwright_cookies_to_netscape(cookies, cookie_file)
                print(f"Saved Playwright cookies to {cookie_file}")
            else:
                print("Playwright failed to get cookies")
    except Exception as e:
        print(f"Could not load/save cookies: {e}")

    ydl_opts = {
        "format": "bestaudio/best",
        "outtmpl": out,
        "writesubtitles": True,
        "writeautomaticsub": True,
        "subtitleslangs": ["zh", "zh-CN", "zh-Hans", "zh-Hant"],
        "skip_download": True,
        "noplaylist": True,
        "http_headers": {
            "Referer": "https://www.bilibili.com",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        },
        "retries": 10,
        "fragment_retries": 10,
        "verbose": True,
    }
    
    if cookie_file:
        ydl_opts["cookiefile"] = cookie_file
    else:
        # Fallback to old method
        ydl_opts["cookiesfrombrowser"] = ("edge", None, None)
        cookie = cookie_header_for(url)
        if cookie:
            ydl_opts["http_headers"]["Cookie"] = cookie

    info = None
    try:
        with YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
    except Exception as e:
        print(f"yt-dlp failed with primary options: {e}")
        # fallback try chrome profile if not already using cookiefile
        if not cookie_file:
            ydl_opts["cookiesfrombrowser"] = ("chrome", None, None)
            try:
                with YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(url, download=True)
            except Exception:
                info = None
    
    title = info.get("title", "") if info else ""
    text = _read_subtitles_text(temp_dir)
    
    # Check if we have an audio file downloaded by yt-dlp
    audio_file = None
    for f in os.listdir(temp_dir):
        if f.endswith((".m4a", ".mp3", ".wav", ".webm")):
            audio_file = os.path.join(temp_dir, f)
            break

    # Fallback to transcription if no subtitles found
    if not text:
        if audio_file:
            print(f"No subtitles found, but audio file exists: {audio_file}. Transcribing...")
            try:
                from ..transcribe.transcriber import Transcriber
                tr = Transcriber(model_size="base")
                text = tr.transcribe(audio_file)
                print("Transcription complete.")
            except Exception as e:
                print(f"Transcription failed: {e}")
        elif info:
            print("No subtitles and no audio file from yt-dlp. Attempting fallback to bilibili-api-python...")
            # Trigger fallback below by setting info to None or handling it explicitly
            # We will let the code fall through to the API fallback if text is still empty
            pass

    # Retry logic for 412 error or missing content using bilibili-api-python
    if not text:
        print("Content missing. Attempting to use bilibili-api-python to get audio stream...")
        try:
            import asyncio
            from bilibili_api import video, Credential
            
            # We need to extract bvid from url
            # https://www.bilibili.com/video/BV1rg411L7sg/?...
            bvid = None
            match = re.search(r"BV[a-zA-Z0-9]+", url)
            if match:
                bvid = match.group(0)
            
            if bvid:
                async def get_audio_url_via_api():
                    # SESSDATA is needed for high quality, but for audio maybe not strictly required for basic
                    # But 412 is IP block or anti-crawl.
                    # We can try without creds first, or use cookies if we have them.
                    # For now, try no creds (or minimal)
                    v = video.Video(bvid=bvid)
                    # We need to get download url
                    # detect cid first
                    info_api = await v.get_info()
                    title_api = info_api.get("title", "")
                    # print(info_api)
                    
                    # Get audio url
                    # https://github.com/Passkou/bilibili-api-python/blob/main/bilibili_api/video.py
                    url_data = await v.get_download_url(page_index=0)
                    # url_data['dash']['audio'][0]['baseUrl']
                    audio_url = None
                    if "dash" in url_data:
                        audios = url_data["dash"].get("audio", [])
                        if audios:
                            audio_url = audios[0].get("baseUrl") or audios[0].get("backupUrl", [None])[0]
                    elif "durl" in url_data:
                         # flv/mp4
                         audio_url = url_data["durl"][0]["url"]
                         
                    return title_api, audio_url

                # Use asyncio.run() which handles loop setup/teardown and defaults to ProactorEventLoop on Windows
                title_api, audio_url = asyncio.run(get_audio_url_via_api())
                
                if title_api:
                    title = title_api
                
                if audio_url:
                    print(f"Got audio URL via API: {audio_url[:50]}...")
                    # Download audio manually
                    audio_path = os.path.join(temp_dir, "audio_temp.m4a")
                    # Use requests or cloudscraper to download
                    import cloudscraper
                    scraper = cloudscraper.create_scraper()
                    headers = {
                        "Referer": "https://www.bilibili.com",
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                    # Streaming download
                    with scraper.get(audio_url, headers=headers, stream=True) as r:
                        r.raise_for_status()
                        with open(audio_path, "wb") as f:
                            for chunk in r.iter_content(chunk_size=8192):
                                f.write(chunk)
                    
                    print(f"Downloaded audio to {audio_path}. Transcribing...")
                    from ..transcribe.transcriber import Transcriber
                    tr = Transcriber(model_size="base")
                    text = tr.transcribe(audio_path)
                    print(f"Transcription complete.")

        except Exception as e:
            print(f"bilibili-api-python fallback failed: {e}")

    # Fallback to transcription if no subtitles found (and yt-dlp worked partially or we just downloaded audio)

    if not text and info:
        desc = info.get("description", "") or info.get("webpage_url_basename", "")
        text = desc
    if not text:
        try:
            from .common import fetch_html
            from bs4 import BeautifulSoup
            html = fetch_html(url)
            soup = BeautifulSoup(html, "html.parser")
            if not title:
                title = soup.title.string if soup.title else ""
            meta_desc = soup.find("meta", attrs={"name": "description"})
            if meta_desc and meta_desc.get("content"):
                text = meta_desc["content"]
        except Exception:
            pass
    return {"title": title, "text": text}


    
